{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember. Persistence reveals the path.\n",
    "\n",
    "For the test we have part of the data from ***A Multilingual, Multi-Style and Multi-Granularity Dataset for Cross-Language Textual Similarity Detection*** *Jeremy Ferrero, Frederic Agnes, Laurent Besacier, Didier Schwab*.\n",
    "\n",
    "Every group of document are separate by category. We have 4 categories:\n",
    "- APR. Amazon Products Reviews.\n",
    "- Conference Papers.\n",
    "- PAN11. Texts come from books freely available on the Gutenberg Project website\n",
    "- Wikipedia.\n",
    "\n",
    "Every category has different folders for each language. This provied data will train the prediction model for **text classification** and **topic model**, so we must determine if all the folders contains enought documents in every language and if they are correct.\n",
    "\n",
    "You could find **cleaning code** in the jupyter notebook file *02_clean_and_language_detection* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Imports\n",
    "Imports the libraries we are going to use in this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. APR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### 1.1. English\n",
    "   \n",
    "   We are going to make a count of the original files in the different folders. I will repeit the count after cleaning to determine if we have enought data for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    }
   ],
   "source": [
    "# Original documents in folder\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/APR/en'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))\n",
    "#print([f for f in glob.glob(path + '/*.txt')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3585\n"
     ]
    }
   ],
   "source": [
    "# After cleaning\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/APR/en'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### 1.2. French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400\n"
     ]
    }
   ],
   "source": [
    "# Original documents in folder\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/APR/fr'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2374\n"
     ]
    }
   ],
   "source": [
    "# After cleaning\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/APR/fr'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Conference_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### 2.1. English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n"
     ]
    }
   ],
   "source": [
    "# Original documents in folder\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/Conference_papers/en'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372\n"
     ]
    }
   ],
   "source": [
    "# After cleaning\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/Conference_papers/en'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### 2.2. French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n"
     ]
    }
   ],
   "source": [
    "# Original documents in folder\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/Conference_papers/fr'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n"
     ]
    }
   ],
   "source": [
    "# After cleaning\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/Conference_papers/fr'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. PAN11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### 3.1. English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1752\n"
     ]
    }
   ],
   "source": [
    "# Original documents in folder\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/PAN11/en'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1752\n"
     ]
    }
   ],
   "source": [
    "# After cleaning\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/PAN11/en'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### 3.2. Spanish\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168\n"
     ]
    }
   ],
   "source": [
    "# Original documents in folder\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/PAN11/es'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168\n"
     ]
    }
   ],
   "source": [
    "# After cleaning\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/PAN11/es'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "# Original documents in folder\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/Wikipedia/en'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3940\n"
     ]
    }
   ],
   "source": [
    "# After cleaning\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/Wikipedia/en'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "# Original documents in folder\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/Wikipedia/es'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3829\n"
     ]
    }
   ],
   "source": [
    "# After cleaning\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/Wikipedia/es'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5588\n"
     ]
    }
   ],
   "source": [
    "# Original documents in folder\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/Wikipedia/fr'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5341\n"
     ]
    }
   ],
   "source": [
    "# After cleaning\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/Wikipedia/fr'\n",
    "print(len([f for f in glob.glob(path + '/*.txt')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Python we have a lot of possibilities to represent the data and make it more easy to undersand. Here, there is an example of how to convert a dictionary to a pandas dataframe with the results of the data identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "      <th>Spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>APR_original</th>\n",
       "      <td>3600.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>APR_clean</th>\n",
       "      <td>3585.0</td>\n",
       "      <td>2374.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conference_papers_original</th>\n",
       "      <td>372.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conference_papers_clean</th>\n",
       "      <td>372.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAN11_original</th>\n",
       "      <td>1752.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAN11_clean</th>\n",
       "      <td>1752.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wikipedia_original</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>5588.0</td>\n",
       "      <td>4000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wikipedia_clean</th>\n",
       "      <td>3940.0</td>\n",
       "      <td>5341.0</td>\n",
       "      <td>3829.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            English  French  Spanish\n",
       "APR_original                 3600.0  2400.0      NaN\n",
       "APR_clean                    3585.0  2374.0      NaN\n",
       "Conference_papers_original    372.0   248.0      NaN\n",
       "Conference_papers_clean       372.0   248.0      NaN\n",
       "PAN11_original               1752.0     NaN   1168.0\n",
       "PAN11_clean                  1752.0     NaN   1168.0\n",
       "Wikipedia_original           4000.0  5588.0   4000.0\n",
       "Wikipedia_clean              3940.0  5341.0   3829.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dic = {'APR_original':{'English': 3600.0, 'French':2400.0},\n",
    "       'APR_clean': {'English':3585.0 , 'French':2374.0},\n",
    "       'Conference_papers_original':{'English': 372.0, 'French':248.0},\n",
    "       'Conference_papers_clean':{'English': 372.0 ,'French':248.0},\n",
    "       'PAN11_original':{'English': 1752.0, 'Spanish': 1168.0},\n",
    "       'PAN11_clean':{'English': 1752.0, 'Spanish': 1168.0},\n",
    "       'Wikipedia_original':{'English': 4000.0, 'French': 5588.0, 'Spanish': 4000.0},\n",
    "       'Wikipedia_clean':{'English':3940.0, 'French': 5341.0, 'Spanish':3829.0}\n",
    "                      }\n",
    "df = pd.DataFrame.from_dict(dic, orient='index')\n",
    "display(df)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions\n",
    "\n",
    "We have enought data to train the model, but we have different amount of files so we need to be carefoul because one category can shadow the rest of the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ea_env]",
   "language": "python",
   "name": "conda-env-ea_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
