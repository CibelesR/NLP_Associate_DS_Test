{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/APR/en/apr-book-0-en.txt'\n",
    "\n",
    "titles = open(data).read().split('\\n')[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/APR/en/apr-book-25-en.txt'\n",
    "\n",
    "titles2 = open(data2).read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'how to solve the disappearance of a singer when one is poor and historians? that is the mission of three young single board unwillingly into a story full of secrets. fred vargas plunges us once again in an original investigation that holds us spellbound by his twists until the last page!'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " titles2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is',\n",
       " 'what',\n",
       " 'your',\n",
       " 'when',\n",
       " 'enough',\n",
       " 'out',\n",
       " 'very',\n",
       " 'meanwhile',\n",
       " 'six',\n",
       " 'a',\n",
       " 'see',\n",
       " 'elsewhere',\n",
       " 'last',\n",
       " 'something',\n",
       " 'here',\n",
       " 'thence',\n",
       " 'afterwards',\n",
       " 'yourselves',\n",
       " 'among',\n",
       " 'throughout',\n",
       " 'besides',\n",
       " 'mine',\n",
       " 'would',\n",
       " '‘s',\n",
       " 'cannot',\n",
       " '’d',\n",
       " 'itself',\n",
       " 'get',\n",
       " 'become',\n",
       " 'everywhere',\n",
       " 'sometimes',\n",
       " 'such',\n",
       " 'therefore',\n",
       " 'due',\n",
       " 'these',\n",
       " '’re',\n",
       " 'doing',\n",
       " 'anyhow',\n",
       " 'either',\n",
       " 'can',\n",
       " 'whereby',\n",
       " 'whither',\n",
       " 'several',\n",
       " 'first',\n",
       " 'so',\n",
       " 'anything',\n",
       " 'namely',\n",
       " 'through',\n",
       " 'quite',\n",
       " 'and',\n",
       " 'himself',\n",
       " 'neither',\n",
       " 'because',\n",
       " 'where',\n",
       " 'too',\n",
       " 'nor',\n",
       " 'myself',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'her',\n",
       " 'give',\n",
       " 'however',\n",
       " 'into',\n",
       " 'other',\n",
       " 'well',\n",
       " 'third',\n",
       " 'before',\n",
       " '‘d',\n",
       " 'how',\n",
       " 'some',\n",
       " 'sixty',\n",
       " 'thru',\n",
       " 'part',\n",
       " 'about',\n",
       " 'forty',\n",
       " 'towards',\n",
       " 'whereafter',\n",
       " 'almost',\n",
       " 'two',\n",
       " 'every',\n",
       " 'each',\n",
       " 'hereby',\n",
       " 'using',\n",
       " 'least',\n",
       " 'am',\n",
       " 'alone',\n",
       " 'his',\n",
       " 'n’t',\n",
       " 'by',\n",
       " 'not',\n",
       " 'unless',\n",
       " 'after',\n",
       " 'than',\n",
       " 'used',\n",
       " \"'ll\",\n",
       " 'really',\n",
       " 'n‘t',\n",
       " 'all',\n",
       " 'few',\n",
       " '‘ve',\n",
       " 'down',\n",
       " 'eight',\n",
       " 'further',\n",
       " \"'re\",\n",
       " 'otherwise',\n",
       " 'which',\n",
       " 'herself',\n",
       " 'via',\n",
       " 'serious',\n",
       " 'hers',\n",
       " 'more',\n",
       " 'i',\n",
       " 'various',\n",
       " 'amongst',\n",
       " 'anyway',\n",
       " 'fifty',\n",
       " 'for',\n",
       " '‘re',\n",
       " 'once',\n",
       " 'noone',\n",
       " 'around',\n",
       " 'why',\n",
       " 'show',\n",
       " 'side',\n",
       " 'us',\n",
       " 'ourselves',\n",
       " 'on',\n",
       " 'have',\n",
       " 'four',\n",
       " 'while',\n",
       " 'make',\n",
       " 'name',\n",
       " 'made',\n",
       " 'whence',\n",
       " 'being',\n",
       " 'done',\n",
       " 'their',\n",
       " 'eleven',\n",
       " 'still',\n",
       " 'any',\n",
       " 're',\n",
       " 'yours',\n",
       " 'was',\n",
       " 'over',\n",
       " 'this',\n",
       " 'or',\n",
       " 'there',\n",
       " 'until',\n",
       " 'per',\n",
       " 'with',\n",
       " 'own',\n",
       " 'together',\n",
       " 'beforehand',\n",
       " 'thereby',\n",
       " 'will',\n",
       " 'although',\n",
       " 'take',\n",
       " 'call',\n",
       " 'please',\n",
       " 'an',\n",
       " 'wherever',\n",
       " 'within',\n",
       " 'whether',\n",
       " 'full',\n",
       " 'many',\n",
       " 'back',\n",
       " 'keep',\n",
       " 'former',\n",
       " 'been',\n",
       " 'that',\n",
       " 'he',\n",
       " 'only',\n",
       " 'go',\n",
       " 'whatever',\n",
       " 'during',\n",
       " 'also',\n",
       " 'bottom',\n",
       " 'just',\n",
       " 'hence',\n",
       " 'our',\n",
       " 'did',\n",
       " 'none',\n",
       " 'latter',\n",
       " 'hereupon',\n",
       " 'beside',\n",
       " 'across',\n",
       " 'whenever',\n",
       " 'from',\n",
       " 'under',\n",
       " 'everyone',\n",
       " 'five',\n",
       " 'sometime',\n",
       " 'move',\n",
       " 'against',\n",
       " 'ours',\n",
       " 'somewhere',\n",
       " 'less',\n",
       " 'say',\n",
       " \"'m\",\n",
       " 'mostly',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'she',\n",
       " '’ll',\n",
       " 'seemed',\n",
       " 'in',\n",
       " 'themselves',\n",
       " 'herein',\n",
       " 'does',\n",
       " 'we',\n",
       " 'thereafter',\n",
       " 'becoming',\n",
       " 'my',\n",
       " \"'d\",\n",
       " 'are',\n",
       " 'above',\n",
       " 'same',\n",
       " 'always',\n",
       " 'nothing',\n",
       " 'the',\n",
       " 'they',\n",
       " 'fifteen',\n",
       " 'already',\n",
       " 'if',\n",
       " 'it',\n",
       " 'upon',\n",
       " '’s',\n",
       " 'became',\n",
       " 'might',\n",
       " 'me',\n",
       " 'beyond',\n",
       " 'him',\n",
       " 'often',\n",
       " 'everything',\n",
       " \"n't\",\n",
       " 'behind',\n",
       " '‘m',\n",
       " 'hundred',\n",
       " 'should',\n",
       " 'below',\n",
       " 'may',\n",
       " 'since',\n",
       " 'amount',\n",
       " 'had',\n",
       " 'anyone',\n",
       " 'but',\n",
       " 'much',\n",
       " 'then',\n",
       " 'wherein',\n",
       " 'to',\n",
       " 'must',\n",
       " 'as',\n",
       " 'whoever',\n",
       " 'thereupon',\n",
       " 'formerly',\n",
       " 'thus',\n",
       " 'yourself',\n",
       " '’ve',\n",
       " 'therein',\n",
       " 'has',\n",
       " 'twelve',\n",
       " 'indeed',\n",
       " 'latterly',\n",
       " 'along',\n",
       " 'yet',\n",
       " 'ten',\n",
       " 'could',\n",
       " 'rather',\n",
       " 'seem',\n",
       " 'ca',\n",
       " 'be',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'do',\n",
       " 'empty',\n",
       " 'someone',\n",
       " 'put',\n",
       " 'without',\n",
       " 'anywhere',\n",
       " 'hereafter',\n",
       " 'onto',\n",
       " 'else',\n",
       " 'up',\n",
       " 'at',\n",
       " 'nine',\n",
       " 'though',\n",
       " 'who',\n",
       " 'next',\n",
       " 'again',\n",
       " 'somehow',\n",
       " 'twenty',\n",
       " 'seems',\n",
       " 'were',\n",
       " 'becomes',\n",
       " \"'ve\",\n",
       " 'between',\n",
       " 'you',\n",
       " 'another',\n",
       " 'them',\n",
       " 'one',\n",
       " '‘ll',\n",
       " 'those',\n",
       " \"'s\",\n",
       " 'perhaps',\n",
       " 'toward',\n",
       " 'nowhere',\n",
       " 'both',\n",
       " 'whereupon',\n",
       " 'except',\n",
       " 'nobody',\n",
       " 'off',\n",
       " 'now',\n",
       " 'regarding',\n",
       " 'front',\n",
       " '’m',\n",
       " 'most',\n",
       " 'seeming',\n",
       " 'others',\n",
       " 'its',\n",
       " 'three',\n",
       " 'no',\n",
       " 'moreover',\n",
       " 'whereas',\n",
       " 'whole',\n",
       " 'of',\n",
       " 'top']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i Lemma: i\n",
      "read Lemma: read\n",
      "this Lemma: this\n",
      "book Lemma: book\n",
      "because Lemma: because\n",
      "in Lemma: in\n",
      "my Lemma: -PRON-\n",
      "town Lemma: town\n",
      ", Lemma: ,\n",
      "everyone Lemma: everyone\n",
      "uses Lemma: use\n",
      "it Lemma: -PRON-\n",
      "and Lemma: and\n",
      "order Lemma: order\n",
      ". Lemma: .\n",
      "this Lemma: this\n",
      "is Lemma: be\n",
      "my Lemma: -PRON-\n",
      "pharmacist Lemma: pharmacist\n",
      "who Lemma: who\n",
      "advised Lemma: advise\n",
      "me Lemma: -PRON-\n",
      "she Lemma: -PRON-\n",
      "was Lemma: be\n",
      "so Lemma: so\n",
      "thin Lemma: thin\n",
      "i Lemma: i\n",
      "asked Lemma: ask\n",
      "her Lemma: -PRON-\n",
      "what Lemma: what\n",
      "she Lemma: -PRON-\n",
      "had Lemma: have\n",
      "done Lemma: do\n",
      "and Lemma: and\n",
      "instead Lemma: instead\n",
      "of Lemma: of\n",
      "just Lemma: just\n",
      "selling Lemma: sell\n",
      "snake Lemma: snake\n",
      "oil Lemma: oil\n",
      "capsules Lemma: capsule\n",
      ", Lemma: ,\n",
      "she Lemma: -PRON-\n",
      "advised Lemma: advise\n",
      "me Lemma: -PRON-\n",
      "this Lemma: this\n",
      "book Lemma: book\n",
      "to Lemma: to\n",
      "5 Lemma: 5\n",
      "euros Lemma: euro\n",
      ". Lemma: .\n",
      "of Lemma: of\n",
      "course Lemma: course\n",
      ", Lemma: ,\n",
      "we Lemma: -PRON-\n",
      "must Lemma: must\n",
      "make Lemma: make\n",
      "an Lemma: an\n",
      "effort Lemma: effort\n",
      "to Lemma: to\n",
      "lose Lemma: lose\n",
      "25 Lemma: 25\n",
      "pounds Lemma: pound\n",
      "but Lemma: but\n",
      "with Lemma: with\n",
      "the Lemma: the\n",
      "book Lemma: book\n",
      ", Lemma: ,\n",
      "i Lemma: i\n",
      "had Lemma: have\n",
      "a Lemma: a\n",
      "companion Lemma: companion\n",
      ". Lemma: .\n",
      "the Lemma: the\n",
      "author Lemma: author\n",
      "was Lemma: be\n",
      "able Lemma: able\n",
      "to Lemma: to\n",
      "talk Lemma: talk\n",
      "to Lemma: to\n",
      "me Lemma: -PRON-\n",
      "just Lemma: just\n",
      "with Lemma: with\n",
      "strong Lemma: strong\n",
      "arguments Lemma: argument\n",
      "and Lemma: and\n",
      "above Lemma: above\n",
      "all Lemma: all\n",
      "i Lemma: i\n",
      "felt Lemma: feel\n",
      "he Lemma: -PRON-\n",
      "knew Lemma: know\n",
      "many Lemma: many\n",
      "cases Lemma: case\n",
      "like Lemma: like\n",
      "mine Lemma: -PRON-\n",
      ". Lemma: .\n",
      "he Lemma: -PRON-\n",
      "is Lemma: be\n",
      "in Lemma: in\n",
      "his Lemma: -PRON-\n",
      "full Lemma: full\n",
      "experience Lemma: experience\n",
      ", Lemma: ,\n",
      "simplicity Lemma: simplicity\n",
      "and Lemma: and\n",
      "compassion Lemma: compassion\n",
      "for Lemma: for\n",
      "those Lemma: those\n",
      "like Lemma: like\n",
      "me Lemma: -PRON-\n",
      "who Lemma: who\n",
      "lived Lemma: live\n",
      "with Lemma: with\n",
      "all Lemma: all\n",
      "that Lemma: that\n",
      "weight Lemma: weight\n",
      "that Lemma: that\n",
      "stuck Lemma: stick\n",
      "to Lemma: to\n",
      "my Lemma: -PRON-\n",
      "body Lemma: body\n",
      "and Lemma: and\n",
      "never Lemma: never\n",
      "want Lemma: want\n",
      "to Lemma: to\n",
      "leave Lemma: leave\n",
      ". Lemma: .\n",
      "i Lemma: i\n",
      "do Lemma: do\n",
      "not Lemma: not\n",
      "think Lemma: think\n",
      "it Lemma: -PRON-\n",
      "is Lemma: be\n",
      "a Lemma: a\n",
      "fad Lemma: fad\n",
      "diet Lemma: diet\n",
      "that Lemma: that\n",
      "outperforms Lemma: outperform\n",
      "the Lemma: the\n",
      "others Lemma: other\n",
      "but Lemma: but\n",
      "i Lemma: i\n",
      "do Lemma: do\n",
      "believe Lemma: believe\n",
      "that Lemma: that\n",
      "there Lemma: there\n",
      "are Lemma: be\n",
      "people Lemma: people\n",
      "who Lemma: who\n",
      "can Lemma: can\n",
      "speak Lemma: speak\n",
      "to Lemma: to\n",
      "others Lemma: other\n",
      "and Lemma: and\n",
      "to Lemma: to\n",
      "be Lemma: be\n",
      "born Lemma: bear\n",
      "of Lemma: of\n",
      "clicks Lemma: click\n",
      ". Lemma: .\n",
      "i Lemma: i\n",
      "might Lemma: may\n",
      "be Lemma: be\n",
      "low Lemma: low\n",
      "but Lemma: but\n",
      "this Lemma: this\n",
      "book Lemma: book\n",
      "made Lemma: make\n",
      "me Lemma: -PRON-\n",
      "strong Lemma: strong\n",
      ", Lemma: ,\n",
      "i Lemma: i\n",
      "have Lemma: have\n",
      "annotated Lemma: annotate\n",
      "so Lemma: so\n",
      "that Lemma: that\n",
      "i Lemma: -PRON-\n",
      "'m Lemma: be\n",
      "on Lemma: on\n",
      "my Lemma: -PRON-\n",
      "third Lemma: third\n",
      ". Lemma: .\n",
      "when Lemma: when\n",
      "one Lemma: one\n",
      "is Lemma: be\n",
      "very Lemma: very\n",
      "big Lemma: big\n",
      "as Lemma: as\n",
      "i Lemma: i\n",
      "was Lemma: be\n",
      ", Lemma: ,\n",
      "non Lemma: non\n",
      "- Lemma: -\n",
      "large Lemma: large\n",
      "do Lemma: do\n",
      "not Lemma: not\n",
      "understand Lemma: understand\n",
      "or Lemma: or\n",
      "are Lemma: be\n",
      "afraid Lemma: afraid\n",
      "to Lemma: to\n",
      "offend Lemma: offend\n",
      "you Lemma: -PRON-\n",
      "by Lemma: by\n",
      "speaking Lemma: speak\n",
      ", Lemma: ,\n",
      "then Lemma: then\n",
      "this Lemma: this\n",
      "book Lemma: book\n",
      "was Lemma: be\n",
      "like Lemma: like\n",
      "a Lemma: a\n",
      "companion Lemma: companion\n",
      "journal Lemma: journal\n",
      ". Lemma: .\n",
      "i Lemma: i\n",
      "am Lemma: be\n",
      "a Lemma: a\n",
      "pedicure Lemma: pedicure\n",
      "and Lemma: and\n",
      "i Lemma: i\n",
      "have Lemma: have\n",
      "advised Lemma: advise\n",
      "all Lemma: all\n",
      "my Lemma: -PRON-\n",
      "clients Lemma: client\n",
      "that Lemma: that\n",
      "i Lemma: i\n",
      "read Lemma: read\n",
      "great Lemma: great\n",
      "suffering Lemma: suffering\n",
      "on Lemma: on\n",
      "their Lemma: -PRON-\n",
      "feet Lemma: foot\n",
      "swollen Lemma: swollen\n",
      "and Lemma: and\n",
      "deformed Lemma: deform\n",
      ". Lemma: .\n",
      "i Lemma: i\n",
      "can Lemma: can\n",
      "provide Lemma: provide\n",
      "other Lemma: other\n",
      "service Lemma: service\n",
      "that Lemma: that\n",
      "i Lemma: i\n",
      "made Lemma: make\n",
      "my Lemma: -PRON-\n",
      "pharmacist Lemma: pharmacist\n",
      ". Lemma: .\n",
      "i Lemma: i\n",
      "advise Lemma: advise\n",
      "all Lemma: all\n",
      "those Lemma: those\n",
      "who Lemma: who\n",
      "suffer Lemma: suffer\n",
      "for Lemma: for\n",
      "having Lemma: have\n",
      "lost Lemma: lose\n",
      "weight Lemma: weight\n",
      "is Lemma: be\n",
      "such Lemma: such\n",
      "a Lemma: a\n",
      "happiness Lemma: happiness\n",
      "that Lemma: that\n",
      "i Lemma: i\n",
      "agreed Lemma: agree\n",
      "to Lemma: to\n",
      "move Lemma: move\n",
      "to Lemma: to\n",
      "phase Lemma: phase\n",
      "3 Lemma: 3\n",
      "of Lemma: of\n",
      "this Lemma: this\n",
      "plan Lemma: plan\n",
      ", Lemma: ,\n",
      "which Lemma: which\n",
      "requires Lemma: require\n",
      "10 Lemma: 10\n",
      "days Lemma: day\n",
      "of Lemma: of\n",
      "consolidation Lemma: consolidation\n",
      "for Lemma: for\n",
      "each Lemma: each\n",
      "kilo Lemma: kilo\n",
      "lost Lemma: lose\n",
      "gradually Lemma: gradually\n",
      "widening Lemma: widen\n",
      "at Lemma: at\n",
      "all Lemma: all\n",
      ". Lemma: .\n",
      "now Lemma: now\n",
      "i Lemma: -PRON-\n",
      "'m Lemma: be\n",
      "in Lemma: in\n",
      "stage Lemma: stage\n",
      "4 Lemma: 4\n",
      ", Lemma: ,\n",
      "meaning Lemma: mean\n",
      "that Lemma: that\n",
      "i Lemma: i\n",
      "eat Lemma: eat\n",
      "everything Lemma: everything\n",
      "except Lemma: except\n",
      "on Lemma: on\n",
      "thursdays Lemma: thursday\n",
      "when Lemma: when\n",
      "i Lemma: i\n",
      "control Lemma: control\n",
      ". Lemma: .\n",
      "i Lemma: i\n",
      "never Lemma: never\n",
      "thank Lemma: thank\n",
      "enough Lemma: enough\n",
      "the Lemma: the\n",
      "author Lemma: author\n",
      "of Lemma: of\n",
      "this Lemma: this\n",
      "book Lemma: book\n",
      ". Lemma: .\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(titles[0])\n",
    "for word in doc:\n",
    "    print(word.text, \"Lemma:\", word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "\n",
    "    tokens = nlp(sentence)\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    for word in tokens:\n",
    "        lemma = word.lemma_.lower().strip()\n",
    "        \n",
    "        if lemma not in STOP_WORDS and word.lemma_ != \"-PRON-\"and re.search('^[a-zA-Z]+$', lemma):\n",
    "            filtered_tokens.append(lemma)\n",
    "\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = spacy_tokenizer(titles[0])\n",
    "doc2 = spacy_tokenizer(titles2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['read',\n",
       " 'book',\n",
       " 'town',\n",
       " 'use',\n",
       " 'order',\n",
       " 'pharmacist',\n",
       " 'advise',\n",
       " 'thin',\n",
       " 'ask',\n",
       " 'instead',\n",
       " 'sell',\n",
       " 'snake',\n",
       " 'oil',\n",
       " 'capsule',\n",
       " 'advise',\n",
       " 'book',\n",
       " 'euro',\n",
       " 'course',\n",
       " 'effort',\n",
       " 'lose',\n",
       " 'pound',\n",
       " 'book',\n",
       " 'companion',\n",
       " 'author',\n",
       " 'able',\n",
       " 'talk',\n",
       " 'strong',\n",
       " 'argument',\n",
       " 'feel',\n",
       " 'know',\n",
       " 'case',\n",
       " 'like',\n",
       " 'experience',\n",
       " 'simplicity',\n",
       " 'compassion',\n",
       " 'like',\n",
       " 'live',\n",
       " 'weight',\n",
       " 'stick',\n",
       " 'body',\n",
       " 'want',\n",
       " 'leave',\n",
       " 'think',\n",
       " 'fad',\n",
       " 'diet',\n",
       " 'outperform',\n",
       " 'believe',\n",
       " 'people',\n",
       " 'speak',\n",
       " 'bear',\n",
       " 'click',\n",
       " 'low',\n",
       " 'book',\n",
       " 'strong',\n",
       " 'annotate',\n",
       " 'big',\n",
       " 'non',\n",
       " 'large',\n",
       " 'understand',\n",
       " 'afraid',\n",
       " 'offend',\n",
       " 'speak',\n",
       " 'book',\n",
       " 'like',\n",
       " 'companion',\n",
       " 'journal',\n",
       " 'pedicure',\n",
       " 'advise',\n",
       " 'client',\n",
       " 'read',\n",
       " 'great',\n",
       " 'suffering',\n",
       " 'foot',\n",
       " 'swollen',\n",
       " 'deform',\n",
       " 'provide',\n",
       " 'service',\n",
       " 'pharmacist',\n",
       " 'advise',\n",
       " 'suffer',\n",
       " 'lose',\n",
       " 'weight',\n",
       " 'happiness',\n",
       " 'agree',\n",
       " 'phase',\n",
       " 'plan',\n",
       " 'require',\n",
       " 'day',\n",
       " 'consolidation',\n",
       " 'kilo',\n",
       " 'lose',\n",
       " 'gradually',\n",
       " 'widen',\n",
       " 'stage',\n",
       " 'mean',\n",
       " 'eat',\n",
       " 'thursday',\n",
       " 'control',\n",
       " 'thank',\n",
       " 'author',\n",
       " 'book']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.871007063548032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Natalio/miniconda3/envs/ea_env/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(nlp(' '.join(doc1)).similarity(nlp(' '.join(doc2))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['read',\n",
       " 'book',\n",
       " 'town',\n",
       " 'use',\n",
       " 'order',\n",
       " 'pharmacist',\n",
       " 'advise',\n",
       " 'thin',\n",
       " 'ask',\n",
       " 'instead',\n",
       " 'sell',\n",
       " 'snake',\n",
       " 'oil',\n",
       " 'capsule',\n",
       " 'advise',\n",
       " 'book',\n",
       " 'euro',\n",
       " 'course',\n",
       " 'effort',\n",
       " 'lose',\n",
       " 'pound',\n",
       " 'book',\n",
       " 'companion',\n",
       " 'author',\n",
       " 'able',\n",
       " 'talk',\n",
       " 'strong',\n",
       " 'argument',\n",
       " 'feel',\n",
       " 'know',\n",
       " 'case',\n",
       " 'like',\n",
       " 'experience',\n",
       " 'simplicity',\n",
       " 'compassion',\n",
       " 'like',\n",
       " 'live',\n",
       " 'weight',\n",
       " 'stick',\n",
       " 'body',\n",
       " 'want',\n",
       " 'leave',\n",
       " 'think',\n",
       " 'fad',\n",
       " 'diet',\n",
       " 'outperform',\n",
       " 'believe',\n",
       " 'people',\n",
       " 'speak',\n",
       " 'bear',\n",
       " 'click',\n",
       " 'low',\n",
       " 'book',\n",
       " 'strong',\n",
       " 'annotate',\n",
       " 'big',\n",
       " 'non',\n",
       " 'large',\n",
       " 'understand',\n",
       " 'afraid',\n",
       " 'offend',\n",
       " 'speak',\n",
       " 'book',\n",
       " 'like',\n",
       " 'companion',\n",
       " 'journal',\n",
       " 'pedicure',\n",
       " 'advise',\n",
       " 'client',\n",
       " 'read',\n",
       " 'great',\n",
       " 'suffering',\n",
       " 'foot',\n",
       " 'swollen',\n",
       " 'deform',\n",
       " 'provide',\n",
       " 'service',\n",
       " 'pharmacist',\n",
       " 'advise',\n",
       " 'suffer',\n",
       " 'lose',\n",
       " 'weight',\n",
       " 'happiness',\n",
       " 'agree',\n",
       " 'phase',\n",
       " 'plan',\n",
       " 'require',\n",
       " 'day',\n",
       " 'consolidation',\n",
       " 'kilo',\n",
       " 'lose',\n",
       " 'gradually',\n",
       " 'widen',\n",
       " 'stage',\n",
       " 'mean',\n",
       " 'eat',\n",
       " 'thursday',\n",
       " 'control',\n",
       " 'thank',\n",
       " 'author',\n",
       " 'book']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenizer(titles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['solve',\n",
       " 'disappearance',\n",
       " 'singer',\n",
       " 'poor',\n",
       " 'historian',\n",
       " 'mission',\n",
       " 'young',\n",
       " 'single',\n",
       " 'board',\n",
       " 'unwillingly',\n",
       " 'story',\n",
       " 'secret',\n",
       " 'fred',\n",
       " 'vargas',\n",
       " 'plunge',\n",
       " 'original',\n",
       " 'investigation',\n",
       " 'hold',\n",
       " 'spellbound',\n",
       " 'twist',\n",
       " 'page']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_tokenizer(titles2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ea_env] *",
   "language": "python",
   "name": "conda-env-ea_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
