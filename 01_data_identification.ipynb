{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are going to start with the differents documents provided.\n",
    "#### Every group of document are separate by language and in paragraph. We must determine if all the documents in the folders are part of the same main document. We are going to do that analyzing the content of the paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob, os\n",
    "\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "STOP_WORDS_en = STOP_WORDS\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "STOP_WORDS_fr = STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### 1. English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "path = '/Users/Natalio/Desktop/nlp_associate_ds_test/NLP_Associate_DS_Test/data/documents_challenge/APR/en'\n",
    "for f in glob.glob(path + '/*.txt'):\n",
    "    df = pd.read_csv(f, sep='\\asdaefsafsadf', header=None, index_col=False ,engine='python')\n",
    "    df['file_name'] = re.findall('.+\\/(.+\\.txt)', f)\n",
    "    df = df.rename(columns={0:'text'})\n",
    "    df_all = pd.concat([df_all,df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i read this book because in my town, everyone ...</td>\n",
       "      <td>apr-book-0-en.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>recipes appreciated by the family (small and l...</td>\n",
       "      <td>apr-book-1-en.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i say no to ease ..... and not to the author w...</td>\n",
       "      <td>apr-book-10-en.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>milady has found a good vein: anita blake. bas...</td>\n",
       "      <td>apr-book-100-en.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>460 bc, somewhere in greece: \"gentlemen, i dec...</td>\n",
       "      <td>apr-book-1000-en.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>i really wonder what is yet to coldplay. i thi...</td>\n",
       "      <td>apr-music-991-en.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>i'm pretty disappointed that it lacks the song...</td>\n",
       "      <td>apr-music-992-en.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>i bought the cd and dvd, it is complemented pe...</td>\n",
       "      <td>apr-music-995-en.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>good but after the disappointing \"follow the l...</td>\n",
       "      <td>apr-music-996-en.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>of course the glory days are behind, but it wo...</td>\n",
       "      <td>apr-music-997-en.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3600 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text             file_name\n",
       "0     i read this book because in my town, everyone ...     apr-book-0-en.txt\n",
       "1     recipes appreciated by the family (small and l...     apr-book-1-en.txt\n",
       "2     i say no to ease ..... and not to the author w...    apr-book-10-en.txt\n",
       "3     milady has found a good vein: anita blake. bas...   apr-book-100-en.txt\n",
       "4     460 bc, somewhere in greece: \"gentlemen, i dec...  apr-book-1000-en.txt\n",
       "...                                                 ...                   ...\n",
       "3595  i really wonder what is yet to coldplay. i thi...  apr-music-991-en.txt\n",
       "3596  i'm pretty disappointed that it lacks the song...  apr-music-992-en.txt\n",
       "3597  i bought the cd and dvd, it is complemented pe...  apr-music-995-en.txt\n",
       "3598  good but after the disappointing \"follow the l...  apr-music-996-en.txt\n",
       "3599  of course the glory days are behind, but it wo...  apr-music-997-en.txt\n",
       "\n",
       "[3600 rows x 2 columns]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "stopwords = list(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(sentence):\n",
    "\n",
    "    tokens = nlp(sentence)\n",
    "    \n",
    "    filtered_tokens = []\n",
    "    for word in tokens:\n",
    "        lemma = word.lemma_.lower().strip()\n",
    "        \n",
    "        if lemma not in STOP_WORDS and word.lemma_ != \"-PRON-\"and re.search('^[a-zA-Z]+$', lemma):\n",
    "            filtered_tokens.append(lemma)\n",
    "\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_similarity(doc1, doc2):\n",
    "    sim_doc1 = spacy_tokenizer(doc1)\n",
    "    sim_doc2 = spacy_tokenizer(doc2)\n",
    "    result = nlp(' '.join(sim_doc1)).similarity(nlp(' '.join(doc2)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-966fb74cbdac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdoc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "doc1 = spacy_tokenizer(titles[0])\n",
    "doc2 = spacy_tokenizer(titles2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.871007063548032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Natalio/miniconda3/envs/ea_env/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_similarity(doc1, doc2):\n",
    "    sim_doc1 = spacy_tokenizer(doc1)\n",
    "    sim_doc2 = spacy_tokenizer(doc2)\n",
    "    result = nlp(' '.join(sim_doc1)).similarity(nlp(' '.join(doc2)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['solve',\n",
       " 'disappearance',\n",
       " 'singer',\n",
       " 'poor',\n",
       " 'historian',\n",
       " 'mission',\n",
       " 'young',\n",
       " 'single',\n",
       " 'board',\n",
       " 'unwillingly',\n",
       " 'story',\n",
       " 'secret',\n",
       " 'fred',\n",
       " 'vargas',\n",
       " 'plunge',\n",
       " 'original',\n",
       " 'investigation',\n",
       " 'hold',\n",
       " 'spellbound',\n",
       " 'twist',\n",
       " 'page']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ea_env] *",
   "language": "python",
   "name": "conda-env-ea_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
